@article{yao2019quality,
  title={Quality of uncertainty quantification for Bayesian neural network inference},
  author={Yao, Jiayu and Pan, Weiwei and Ghosh, Soumya and Doshi-Velez, Finale},
  journal={arXiv preprint arXiv:1906.09686},
  year={2019}
}

@article{matthews2018gaussian,
  title={Gaussian process behaviour in wide deep neural networks},
  author={Matthews, Alexander G de G and Rowland, Mark and Hron, Jiri and Turner, Richard E and Ghahramani, Zoubin},
  journal={arXiv preprint arXiv:1804.11271},
  year={2018}
}

@article{gilmer2018motivating,
  title={Motivating the rules of the game for adversarial example research},
  author={Gilmer, Justin and Adams, Ryan P and Goodfellow, Ian and Andersen, David and Dahl, George E},
  journal={arXiv preprint arXiv:1807.06732},
  year={2018}
}

@article{lipton2018troubling,
  title={Troubling trends in machine learning scholarship},
  author={Lipton, Zachary C and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:1807.03341},
  year={2018}
}

@inproceedings{mou-etal-2019-discreteness,
    title = "Discreteness in Neural Natural Language Processing",
    author = "Mou, Lili  and
      Zhou, Hao  and
      Li, Lei",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    % publisher = "Association for Computational Linguistics",
    % url = "https://www.aclweb.org/anthology/D19-2005",
    abstract = "This tutorial provides a comprehensive guide to the process of discreteness in neural NLP.As a gentle start, we will briefly introduce the background of deep learning based NLP, where we point out the ubiquitous discreteness of natural language and its challenges in neural information processing. Particularly, we will focus on how such discreteness plays a role in the input space, the latent space, and the output space of a neural network. In each part, we will provide examples, discuss machine learning techniques, as well as demonstrate NLP applications.''",
}

@article{belkin2018reconciling,
  title={Reconciling modern machine-learning practice and the classical bias--variance trade-off},
  author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={32},
  pages={15849--15854},
  year={2019},
  publisher={National Acad Sciences}
}

@article{wan2019long,
  title={Long-length Legal Document Classification},
  author={Wan, Lulu and Papageorgiou, George and Seddon, Michael and Bernardoni, Mirko},
  journal={arXiv preprint arXiv:1912.06905},
  year={2019}
}

@inproceedings{chen2019large,
    title = "How Large a Vocabulary Does Text Classification Need? A Variational Approach to Vocabulary Selection",
    author = "Chen, Wenhu  and
      Su, Yu  and
      Shen, Yilin  and
      Chen, Zhiyu  and
      Yan, Xifeng  and
      Wang, William Yang",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    % publisher = "Association for Computational Linguistics",
    % url = "https://www.aclweb.org/anthology/N19-1352",
    % doi = "10.18653/v1/N19-1352",
    pages = "3487--3497",
    abstract = "With the rapid development in deep learning, deep neural networks have been widely adopted in many real-life natural language applications. Under deep neural networks, a pre-defined vocabulary is required to vectorize text inputs. The canonical approach to select pre-defined vocabulary is based on the word frequency, where a threshold is selected to cut off the long tail distribution. However, we observed that such a simple approach could easily lead to under-sized vocabulary or over-sized vocabulary issues. Therefore, we are interested in understanding how the end-task classification accuracy is related to the vocabulary size and what is the minimum required vocabulary size to achieve a specific performance. In this paper, we provide a more sophisticated variational vocabulary dropout (VVD) based on variational dropout to perform vocabulary selection, which can intelligently select the subset of the vocabulary to achieve the required performance. To evaluate different algorithms on the newly proposed vocabulary selection problem, we propose two new metrics: Area Under Accuracy-Vocab Curve and Vocab Size under X{\%} Accuracy Drop. Through extensive experiments on various NLP classification tasks, our variational framework is shown to significantly outperform the frequency-based and other selection baselines on these metrics.",
}

@inproceedings{roth1998learning,
  title={Learning to resolve natural language ambiguities: A unified approach},
  author={Roth, Dan},
  booktitle={AAAI/IAAI},
  pages={806--813},
  year={1998}
}

@article{berger-etal-1996-maximum,
    title = "A Maximum Entropy Approach to Natural Language Processing",
    author = "Berger, Adam L.  and
      Della Pietra, Stephen A.  and
      Della Pietra, Vincent J.",
    journal = "Computational Linguistics",
    volume = "22",
    number = "1",
    year = "1996",
    url = "https://www.aclweb.org/anthology/J96-1002",
    pages = "39--71",
}

@article{gal2018sufficient,
  title={Sufficient conditions for idealised models to have no adversarial examples: a theoretical and empirical study with Bayesian neural networks},
  author={Gal, Yarin and Smith, Lewis},
  journal={arXiv preprint arXiv:1806.00667},
  year={2018}
}
@article{karpas2017information,
  title={Information socialtaxis and efficient collective behavior emerging in groups of information-seeking agents},
  author={Karpas, Ehud D and Shklarsh, Adi and Schneidman, Elad},
  journal={Proceedings of the National Academy of Sciences},
  volume={114},
  number={22},
  pages={5589--5594},
  year={2017},
  publisher={National Acad Sciences}
}

@article{wilks2010sampling,
  title={Sampling distributions of the Brier score and Brier skill score under serial dependence},
  author={Wilks, Daniel S},
  journal={Quarterly Journal of the Royal Meteorological Society},
  volume={136},
  number={653},
  pages={2109--2118},
  year={2010},
  publisher={Wiley Online Library}
}

@article{jospin2020hands,
  title={Hands-on Bayesian Neural Networks--a Tutorial for Deep Learning Users},
  author={Jospin, Laurent Valentin and Buntine, Wray and Boussaid, Farid and Laga, Hamid and Bennamoun, Mohammed},
  journal={arXiv preprint arXiv:2007.06823},
  year={2020}
}

@article{kendall2015bayesian,
  title={Bayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding},
  author={Kendall, Alex and Badrinarayanan, Vijay and Cipolla, Roberto},
  journal={arXiv preprint arXiv:1511.02680},
  year={2015}
}

@inproceedings{quinonero2005evaluating,
  title={Evaluating predictive uncertainty challenge},
  author={Quinonero-Candela, Joaquin and Rasmussen, Carl Edward and Sinz, Fabian and Bousquet, Olivier and Sch{\"o}lkopf, Bernhard},
  booktitle={Machine Learning Challenges Workshop},
  pages={1--27},
  year={2005},
  organization={Springer}
}

@article{murphy1970scoring,
author = "Allan H. Murphy and Robert L. Winkler",
title = "Scoring rules in probability assessment and evaluation",
journal = "Acta Psychologica",
volume = "34",
pages = "273 - 286",
year = "1970",
% issn = "0001-6918",
% doi = "https://doi.org/10.1016/0001-6918(70)90023-5",
% url = "http://www.sciencedirect.com/science/article/pii/0001691870900235",
}


@inproceedings{davis2006relationship,
  title={The relationship between Precision-Recall and {ROC} curves},
  author={Davis, Jesse and Goadrich, Mark},
  booktitle={Proceedings of the 23rd International Conference on Machine Learning},
  pages={233--240},
  year={2006}
}

@article{hendrycks2020many,
  title={The many faces of robustness: A critical analysis of out-of-distribution generalization},
  author={Hendrycks, Dan and Basart, Steven and Mu, Norman and Kadavath, Saurav and Wang, Frank and Dorundo, Evan and Desai, Rahul and Zhu, Tyler and Parajuli, Samyak and Guo, Mike and others},
  journal={arXiv preprint arXiv:2006.16241},
  year={2020}
}

@inproceedings{sun2019fine,
  title={How to fine-tune bert for text classification?},
  author={Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
  booktitle={China National Conference on Chinese Computational Linguistics},
  pages={194--206},
  year={2019},
  organization={Springer}
}

@PhdThesis{Gal2016Uncertainty,
  title={Uncertainty in Deep Learning},
  author={Gal, Yarin},
  year={2016},
  school={University of Cambridge}
}

@InCollection{turnersahani2011,
  author = 	 {R. E. Turner and M. Sahani},
  title = 	 {Two problems with variational expectation
                  maximisation for time-series models},
  booktitle = 	 {Bayesian Time series models},
  pages = 	 {109--130},
  publisher = {Cambridge University Press},
  year = 	 2011,
  editor = 	 {D. Barber and T. Cemgil and S. Chiappa},
  chapter = 	 5,
  abstract ={Variational methods are a key component of the
                  approximate inference and learning toolbox. These
                  methods fill an important middle ground, retaining
                  distributional information about uncertainty in
                  latent variables, unlike maximum a posteriori
                  methods (MAP), and yet generally requiring less
                  computational time than Monte Carlo Markov Chain
                  methods. In particular the variational Expectation
                  Maximisation (vEM) and variational Bayes algorithms,
                  both involving variational optimisation of a
                  free-energy, are widely used in time-series
                  modelling. Here, we investigate the success of vEM
                  in simple probabilistic time-series models. First we
                  consider the inference step of vEM, and show that a
                  consequence of the well-known compactness property
                  of variational inference is a failure to propagate
                  uncertainty in time, thus limiting the usefulness of
                  the retained distributional information. In
                  particular, the uncertainty may appear to be
                  smallest precisely when the approximation is
                  poorest. Second, we consider parameter learning and
                  analytically reveal systematic biases in the
                  parameters found by vEM. Surprisingly, simpler
                  variational approximations (such a mean-field) can
                  lead to less bias than more complicated structured
                  approximations.  }
}

@inproceedings{pearce2018uncertainty,
  title={Uncertainty in neural networks: Approximately Bayesian ensembling},
  author={Pearce, Tim and Leibfried, Felix and Brintrup, Alexandra},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={234--244},
  year={2020},
  organization={PMLR}
}

@inproceedings{graves2011practical,
  title={Practical variational inference for neural networks},
  author={Graves, Alex},
  booktitle={Advances in neural information processing systems},
  pages={2348--2356},
  year={2011}
}

@article{brazowski_collective_2020,
	title = {Collective Learning by Ensembles of Altruistic Diversifying Neural Networks},
	abstract = {Combining the predictions of collections of neural networks often outperforms the best single network. Such ensembles are typically trained independently, and their superior `wisdom of the crowd' originates from the differences between networks. Collective foraging and decision making in socially interacting animal groups is often improved or even optimal thanks to local information sharing between conspecifics. We therefore present a model for co-learning by ensembles of interacting neural networks that aim to maximize their own performance but also their functional relations to other networks. We show that ensembles of interacting networks outperform independent ones, and that optimal ensemble performance is reached when the coupling between networks increases diversity and degrades the performance of individual networks. Thus, even without a global goal for the ensemble, optimal collective behavior emerges from local interactions between networks. We show the scaling of optimal coupling strength with ensemble size, and that networks in these ensembles specialize functionally and become more `confident' in their assessments. Moreover, optimal co-learning networks differ structurally, relying on sparser activity, a wider range of synaptic weights, and higher firing rates - compared to independently trained networks. Finally, we explore interactions-based co-learning as a framework for expanding and boosting ensembles.},
	urldate = {2020-08-21},
	journal = {arXiv:2006.11671 [cs, stat]},
	author = {Brazowski, Benjamin and Schneidman, Elad},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.11671},
	keywords = {Computer Science - Machine Learning, Computer Science - Multiagent Systems, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jordy/Zotero/storage/B4PPSXC8/Brazowski and Schneidman - 2020 - Collective Learning by Ensembles of Altruistic Div.pdf:application/pdf;arXiv.org Snapshot:/home/jordy/Zotero/storage/YD8WGGA3/2006.html:text/html}
}

@article{liu2020simple,
  title={Simple and Principled Uncertainty Estimation with Deterministic Deep Learning via Distance Awareness},
  author={Liu, Jeremiah Zhe and Lin, Zi and Padhy, Shreyas and Tran, Dustin and Bedrax-Weiss, Tania and Lakshminarayanan, Balaji},
  journal={arXiv preprint arXiv:2006.10108},
  year={2020}
}

@inproceedings{jain2020maximizing,
  title={Maximizing Overall Diversity for Improved Uncertainty Estimates in Deep Ensembles.},
  author={Jain, Siddhartha and Liu, Ge and Mueller, Jonas and Gifford, David},
  journal={The Thirty-Fourth AAAI Conference on Artificial Intelligence},
  pages={4264--4271},
  year={2020}
}
@article{tsymbalov2020dropout,
  title={Dropout Strikes Back: Improved Uncertainty Estimation via Diversity Sampled Implicit Ensembles},
  author={Tsymbalov, Evgenii and Fedyanin, Kirill and Panov, Maxim},
  journal={arXiv preprint arXiv:2003.03274},
  year={2020}
}
@article{mukhoti2018importance,
  title={On the importance of strong baselines in Bayesian deep learning},
  author={Mukhoti, Jishnu and Stenetorp, Pontus and Gal, Yarin},
  journal={arXiv preprint arXiv:1811.09385},
  year={2018}
}

@article{dolan2002benchmarking,
  title={Benchmarking optimization software with performance profiles},
  author={Dolan, Elizabeth D and Mor{\'e}, Jorge J},
  journal={Mathematical programming},
  volume={91},
  number={2},
  pages={201--213},
  year={2002},
  publisher={Springer}
}


@inproceedings{rasmussen2003gaussian,
  title={Gaussian processes in machine learning},
  author={Rasmussen, Carl Edward},
  booktitle={Summer School on Machine Learning},
  pages={63--71},
  year={2003},
  organization={Springer}
}


@article{miok2020ban,
  title={To BAN or not to BAN: Bayesian Attention Networks for Reliable Hate Speech Detection},
  author={Miok, Kristian and Skrlj, Blaz and Zaharie, Daniela and Robnik-Sikonja, Marko},
  journal={arXiv preprint arXiv:2007.05304},
  year={2020}
}

@inproceedings{yang2018sgm,
    title = "{SGM}: Sequence Generation Model for Multi-label Classification",
    author = "Yang, Pengcheng  and
      Sun, Xu  and
      Li, Wei  and
      Ma, Shuming  and
      Wu, Wei  and
      Wang, Houfeng",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    % publisher = "Association for Computational Linguistics",
    % url = "https://www.aclweb.org/anthology/C18-1330",
    pages = "3915--3926",
}

@inproceedings{larson2019evaluation,
    title = "An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction",
    author = "Larson, Stefan  and
      Mahendran, Anish  and
      Peper, Joseph J.  and
      Clarke, Christopher  and
      Lee, Andrew  and
      Hill, Parker  and
      Kummerfeld, Jonathan K.  and
      Leach, Kevin  and
      Laurenzano, Michael A.  and
      Tang, Lingjia  and
      Mars, Jason",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    % publisher = "Association for Computational Linguistics",
    % url = "https://www.aclweb.org/anthology/D19-1131",
    % doi = "10.18653/v1/D19-1131",
    pages = "1311--1316",
}

@article{larson2019evaluation,
  title={An evaluation dataset for intent classification and out-of-scope prediction},
  author={Larson, Stefan and Mahendran, Anish and Peper, Joseph J and Clarke, Christopher and Lee, Andrew and Hill, Parker and Kummerfeld, Jonathan K and Leach, Kevin and Laurenzano, Michael A and Tang, Lingjia and others},
  journal={arXiv preprint arXiv:1909.02027},
  year={2019}
}

@article{pal2020multi,
  title={Multi-Label Text Classification using Attention-based Graph Neural Network},
  author={Pal, Ankit and Selvakumar, Muru and Sankarasubbu, Malaikannan},
  journal={arXiv preprint arXiv:2003.11644},
  year={2020}
}

@inproceedings{adhikari2019docbert,
  title={Exploring the Limits of Simple Learners in Knowledge Distillation for Document Classification with DocBERT},
  author={Adhikari, Ashutosh and Ram, Achyudh and Tang, Raphael and Hamilton, William L and Lin, Jimmy},
  booktitle={Proceedings of the 5th Workshop on Representation Learning for NLP},
  pages={72--77},
  year={2020}
}


@article{lang199520,
author = "Ken Lang",
  title="Newsweeder: Learning to filter netnews. version 20news-18828",
  journal = "Machine Learning Proceedings 1995",
address = "San Francisco (CA)",
pages = "331 - 339",
year = "1995"
% publisher = "Morgan Kaufmann",
% isbn = "978-1-55860-377-6",
% doi = "https://doi.org/10.1016/B978-1-55860-377-6.50048-7",
% url = "http://www.sciencedirect.com/science/article/pii/B9781558603776500487",
}

@misc{TFDS,
  title = {{TensorFlow Datasets}, A collection of ready-to-use datasets},
  howpublished = {\url{https://www.tensorflow.org/datasets}},
}

@inproceedings{hendrycks2020pretrained,
    title = "Pretrained Transformers Improve Out-of-Distribution Robustness",
    author = "Hendrycks, Dan  and
      Liu, Xiaoyuan  and
      Wallace, Eric  and
      Dziedzic, Adam  and
      Krishnan, Rishabh  and
      Song, Dawn",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    % address = "Online",
    % publisher = "Association for Computational Linguistics",
    % url = "https://www.aclweb.org/anthology/2020.acl-main.244",
    % doi = "10.18653/v1/2020.acl-main.244",
    pages = "2744--2751",
    abstract = "Although pretrained Transformers such as BERT achieve high accuracy on in-distribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained Transformers{'} performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness.",
}


@article{hendrycks2020pretrained,
  title={Pretrained transformers improve out-of-distribution robustness},
  author={Hendrycks, Dan and Liu, Xiaoyuan and Wallace, Eric and Dziedzic, Adam and Krishnan, Rishabh and Song, Dawn},
  journal={arXiv preprint arXiv:2004.06100},
  year={2020}
}

@inproceedings{devlin2018bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    % publisher = "Association for Computational Linguistics",
    % url = "https://www.aclweb.org/anthology/N19-1423",
    % doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@inproceedings{mukherjee2020uncertainty,
    title = "Uncertainty-aware Self-training for Few-shot Text Classification",
    author = "Mukherjee, Subhabrata  and
      Hassan Awadallah, Ahmed",
    booktitle = "Advances in Neural Information Processing Systems",
    year = "2020",
    %address = "Online",
    % url = "https://papers.nips.cc/paper/2020/file/f23d125da1e29e34c552f448610ff25f-Paper.pdf",
}

@article{byrne2016note,
  title={A note on the use of empirical AUC for evaluating probabilistic forecasts},
  author={Byrne, Simon and others},
  journal={Electronic Journal of Statistics},
  volume={10},
  number={1},
  pages={380--393},
  year={2016},
  publisher={The Institute of Mathematical Statistics and the Bernoulli Society}
}
@article{hernandez2012unified,
  title={A unified view of performance metrics: translating threshold choice into expected classification loss},
  author={Hern{\'a}ndez-Orallo, Jos{\'e} and Flach, Peter and Ferri, C{\`e}sar},
  journal={The Journal of Machine Learning Research},
  volume={13},
  number={1},
  pages={2813--2869},
  year={2012},
  publisher={JMLR. org}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5998--6008},
  year={2017}
}

@article{zhang2020revisiting,
  title={Revisiting Few-sample BERT Fine-tuning},
  author={Zhang, Tianyi and Wu, Felix and Katiyar, Arzoo and Weinberger, Kilian Q and Artzi, Yoav},
  journal={arXiv preprint arXiv:2006.05987},
  year={2020}
}

@phdthesis{mackay1992bayesian,
  title={Bayesian Methods for Adaptive Models},
  author={MacKay, David JC},
  year={1992},
  school={California Institute of Technology}
}
@article{denker1987large,
  title={Large automatic learning, rule extraction, and generalization},
  author={Denker, John and Schwartz, Daniel and Wittner, Ben and Solla, Sara and Howard, Richard and Jackel, Lawrence and Hopfield, John},
  journal={Complex Systems},
  volume={1},
  number={5},
  pages={877--922},
  year={1987}
}
@incollection{neal1992bayesian,
  title={Bayesian mixture modeling},
  author={Neal, Radford M},
  booktitle={Maximum Entropy and Bayesian Methods},
  pages={197--211},
  year={1992},
  publisher={Springer}
}

@inproceedings{ghahramani2016history,
  title={A history of bayesian neural networks},
  author={Ghahramani, Zoubin},
  booktitle={NIPS Workshop on Bayesian Deep Learning},
  year={2016}
}
@inproceedings{hernandez2015probabilistic,
  title={Probabilistic backpropagation for scalable learning of bayesian neural networks},
  author={Hern{\'a}ndez-Lobato, Jos{\'e} Miguel and Adams, Ryan},
  booktitle={International Conference on Machine Learning},
  pages={1861--1869},
  year={2015}
}
@inproceedings{hinton1993keeping,
  title={Keeping the neural networks simple by minimizing the description length of the weights},
  author={Hinton, Geoffrey E and Van Camp, Drew},
  booktitle={Proceedings of the Sixth Annual Conference on Computational Learning Theory},
  pages={5--13},
  year={1993}
}

@article{blundell2015weight,
  title={Weight uncertainty in neural networks},
  author={Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
  journal={arXiv preprint arXiv:1505.05424},
  year={2015}
}

@article{papamarkou2019challenges,
  title={Challenges in Bayesian inference via Markov chain Monte Carlo for neural networks},
  author={Papamarkou, Theodore and Hinkle, Jacob and Young, M Todd and Womble, David},
  journal={arXiv preprint arXiv:1910.06539},
  year={2019}
}

@article{hullermeier2019aleatoric,
  title={Aleatoric and epistemic uncertainty in machine learning: A tutorial introduction},
  author={H{\"u}llermeier, Eyke and Waegeman, Willem},
  journal={arXiv preprint arXiv:1910.09457},
  year={2019}
}

@article{fort2019deep,
  title={Deep ensembles: A loss landscape perspective},
  author={Fort, Stanislav and Hu, Huiyi and Lakshminarayanan, Balaji},
  journal={arXiv preprint arXiv:1912.02757},
  year={2019}
}

%here starts the old:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{kwonuncertaintyclassification,
  title={Uncertainty quantification using Bayesian neural networks in classification: Application to biomedical image segmentation},
  author={Kwon, Yongchan and Won, Joong-Ho and Kim, Beom Joon and Paik, Myunghee Cho},
  journal={Computational Statistics \& Data Analysis},
  volume={142},
  pages={106816},
  year={2020},
  publisher={Elsevier}
}

@article{kwonuncertaintyclassification,
	title = {Uncertainty quantiﬁcation using Bayesian neural networks in classiﬁcation},
	author = {Kwon, Yongchan and Won, Joong-Ho and Kim, Beom Joon and Paik, Myunghee Cho},
	pages = {13},
    year = {2018}
}

@article{pimentel_review_2014,
  title={A Review of Novelty Detection},
  author={Pimentel, Marco AF and Clifton, David A and Clifton, Lei and Tarassenko, Lionel},
  journal={Signal Processing},
  volume={99},
  pages={215--249},
  year={2014},
  publisher={Elsevier}
}

@inproceedings{Hron2018VariationalBD,
  title={Variational Bayesian dropout: pitfalls and fixes},
  author={Jiri Hron and Alexander G. de G. Matthews and Zoubin Ghahramani},
  booktitle={ICML},
  year={2018}
}


@article{hendrycks2016baseline,
  title={A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal = {5th International Conference on Learning Representations},
  year={2017}
}

@misc{rawat2017adversarial,
    title={Adversarial Phenomenon in the Eyes of Bayesian Deep Learning},
    author={Ambrish Rawat and Martin Wistuba and Maria-Irina Nicolae},
    year={2017},
    eprint={1711.08244},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@misc{feinman2017detecting,
    title={Detecting Adversarial Samples from Artifacts},
    author={Reuben Feinman and Ryan R. Curtin and Saurabh Shintre and Andrew B. Gardner},
    year={2017},
    eprint={1703.00410},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}


@incollection{malinin_predictive_2018,
	title = {Predictive {Uncertainty} {Estimation} via {Prior} {Networks}},
	url = {http://papers.nips.cc/paper/7936-predictive-uncertainty-estimation-via-prior-networks.pdf},
	urldate = {2020-05-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Malinin, Andrey and Gales, Mark},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {7047--7058},
	file = {NIPS Full Text PDF:/home/jordy/snap/zotero-snap/common/Zotero/storage/ZLJ7DTX5/Malinin and Gales - 2018 - Predictive Uncertainty Estimation via Prior Networ.pdf:application/pdf;NIPS Snapshot:/home/jordy/snap/zotero-snap/common/Zotero/storage/H6BLC6AM/7936-predictive-uncertainty-estimation-via-prior-networks.html:text/html}
}


@misc{labach2019survey,
    title={Survey of Dropout Methods for Deep Neural Networks},
    author={Alex Labach and Hojjat Salehinejad and Shahrokh Valaee},
    year={2019},
    eprint={1904.13310},
    archivePrefix={arXiv},
    primaryClass={cs.NE}
}

@article{Zhu_2017,
   title={Deep and Confident Prediction for Time Series at Uber},
   ISBN={9781538638002},
   url={http://dx.doi.org/10.1109/ICDMW.2017.19},
   DOI={10.1109/icdmw.2017.19},
   journal={2017 IEEE International Conference on Data Mining Workshops (ICDMW)},
   publisher={IEEE},
   author={Zhu, Lingxue and Laptev, Nikolay},
   year={2017},
   month={Nov}
}


@misc{devries2018learning,
    title={Learning Confidence for Out-of-Distribution Detection in Neural Networks},
    author={Terrance DeVries and Graham W. Taylor},
    year={2018},
    eprint={1802.04865},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}


@misc{ma2019nlpaug,
  title={NLP Augmentation},
  author={Edward Ma},
  howpublished={\url{https://github.com/makcedward/nlpaug}},
  year={2019}
}

 @misc{blogpost,
  title={Bayesian Neural Networks Need Not Concentrate},
  author={Gelada, Carles and Buckman, Jacob},
  howpublished={\url{https://jacobbuckman.com/2020-01-22-bayesian-neural-networks-need-not-concentrate/}},
  year={2020}
}

@article{foong2019expressiveness,
  title={On the expressiveness of approximate inference in bayesian neural networks},
  author={Foong, Andrew and Burt, David and Li, Yingzhen and Turner, Richard},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@inproceedings{kim_convolutional_2014,
    title = "Convolutional Neural Networks for Sentence Classification",
    author = "Kim, Yoon",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    % publisher = "Association for Computational Linguistics",
    % url = "https://www.aclweb.org/anthology/D14-1181",
    % doi = "10.3115/v1/D14-1181",
    pages = "1746--1751",
}

@article{wilson_case_2020,
	title = {The {Case} for {Bayesian} {Deep} {Learning}},
  author={Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:2001.10995},
  year={2020}
}

@inproceedings{gal_dropout_2016,
  title={Dropout as a bayesian approximation: Representing model uncertainty in deep learning},
  author={Gal, Yarin and Ghahramani, Zoubin},
  booktitle={International Conference on Machine Learning},
  pages={1050--1059},
  year={2016}
}

@article{maddox_simple_2019,
	title = {A {Simple} {Baseline} for {Bayesian} {Uncertainty} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/1902.02476},
	abstract = {We propose SWA-Gaussian (SWAG), a simple, scalable, and general purpose approach for uncertainty representation and calibration in deep learning. Stochastic Weight Averaging (SWA), which computes the first moment of stochastic gradient descent (SGD) iterates with a modified learning rate schedule, has recently been shown to improve generalization in deep learning. With SWAG, we fit a Gaussian using the SWA solution as the first moment and a low rank plus diagonal covariance also derived from the SGD iterates, forming an approximate posterior distribution over neural network weights; we then sample from this Gaussian distribution to perform Bayesian model averaging. We empirically find that SWAG approximates the shape of the true posterior, in accordance with results describing the stationary distribution of SGD iterates. Moreover, we demonstrate that SWAG performs well on a wide variety of tasks, including out of sample detection, calibration, and transfer learning, in comparison to many popular alternatives including MC dropout, KFAC Laplace, SGLD, and temperature scaling.},
	urldate = {2020-01-23},
	journal = {arXiv:1902.02476 [cs, stat]},
	author = {Maddox, Wesley and Garipov, Timur and Izmailov, Pavel and Vetrov, Dmitry and Wilson, Andrew Gordon},
	month = dec,
	year = {2019},
	note = {arXiv: 1902.02476
version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/jordy/snap/zotero-snap/common/Zotero/storage/KLZ2ZK8A/1902.html:text/html;arXiv Fulltext PDF:/home/jordy/snap/zotero-snap/common/Zotero/storage/VZ4NPIWS/Maddox et al. - 2019 - A Simple Baseline for Bayesian Uncertainty in Deep.pdf:application/pdf}
}

@article{srivastava_dropout,
	title = {Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
%   publisher={JMLR. org}
}

@inproceedings{lee2018simple,
  title={A simple unified framework for detecting out-of-distribution samples and adversarial attacks},
  author={Lee, Kimin and Lee, Kibok and Lee, Honglak and Shin, Jinwoo},
  booktitle={Advances in Neural Information Processing Systems},
  pages={7167--7177},
  year={2018}
}

@Article{o_know_2018,
	title = {Know {When} {You} {Don}'t {Know}: {A} {Robust} {Deep} {Learning} {Approach} in the {Presence} of {Unknown} {Phenotypes}},
  author={D{\"u}rr, Oliver and Murina, Elvis and Siegismund, Daniel and Tolkachev, Vasily and Steigele, Stephan and Sick, Beate},
  journal={Assay and drug development technologies},
  volume={16},
  number={6},
  pages={343--349},
  year={2018},
  publisher={Mary Ann Liebert, Inc. 140 Huguenot Street, 3rd Floor New Rochelle, NY 10801 USA}
}
@incollection{krogh_simple_1992,
	title = {A {Simple} {Weight} {Decay} {Can} {Improve} {Generalization}},
	urldate = {2020-04-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 4},
	publisher = {Morgan-Kaufmann},
	author = {Krogh, Anders and Hertz, John A.},
	editor = {Moody, J. E. and Hanson, S. J. and Lippmann, R. P.},
	year = {1992},
	pages = {950--957},
	file = {NIPS Full Text PDF:/home/jordy/snap/zotero-snap/common/Zotero/storage/FWC7FQR3/Krogh and Hertz - 1992 - A Simple Weight Decay Can Improve Generalization.pdf:application/pdf;NIPS Snapshot:/home/jordy/snap/zotero-snap/common/Zotero/storage/CE4RA2IT/563-a-simple-weight-decay-can-improve-generalization.html:text/html}
}

@article{bishop1994novelty,
  title={{Novelty Detection and Neural Network Validation}},
  author={Bishop, Christopher M},
  journal={IEE Proceedings-Vision, Image and Signal processing},
  volume={141},
  number={4},
  pages={217--222},
  year={1994},
  publisher={IET}
}

@misc{geifman2018biasreduced,
    title={Bias-Reduced Uncertainty Estimation for Deep Neural Classifiers},
    author={Yonatan Geifman and Guy Uziel and Ran El-Yaniv},
    year={2018},
    eprint={1805.08206},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{sun2019functional,
  title={Functional Variational Bayesian Neural Networks},
  author={Sun, Shengyang and Zhang, Guodong and Shi, Jiaxin and Grosse, Roger},
  journal={arXiv preprint arXiv:1903.05779},
  year={2019}
}
@misc{louizos2017multiplicative,
    title={Multiplicative Normalizing Flows for Variational Bayesian Neural Networks},
    author={Christos Louizos and Max Welling},
    year={2017},
    eprint={1703.01961},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}
@misc{bui2016deep,
    title={Deep Gaussian Processes for Regression using Approximate Expectation Propagation},
    author={Thang D. Bui and Daniel Hernández-Lobato and Yingzhen Li and José Miguel Hernández-Lobato and Richard E. Turner},
    year={2016},
    eprint={1602.04133},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}


@article{leibig_leveraging_2017,
  title={Leveraging Uncertainty Information from Deep Neural Networks for Disease Detection},
  author={Leibig, Christian and Allken, Vaneeda and Ayhan, Murat Se{\c{c}}kin and Berens, Philipp and Wahl, Siegfried},
  journal={Scientific reports},
  volume={7},
  number={1},
  pages={1--14},
  year={2017},
  publisher={Nature Publishing Group}
}

@incollection{tanwani2009classification,
  title={Classification Potential vs. Classification Accuracy: a Comprehensive Study of Evolutionary Algorithms with Biomedical Datasets},
  author={Tanwani, Ajay Kumar and Farooq, Muddassar},
  booktitle={Learning Classifier Systems},
  pages={127--144},
  year={2009},
  publisher={Springer}
}


@article{geweke_analysis_2014,
	title = {Analysis of {Variance} for {Bayesian} {Inference}},
	volume = {33},
	issn = {0747-4938, 1532-4168},
	url = {http://www.tandfonline.com/doi/abs/10.1080/07474938.2013.807182},
	doi = {10.1080/07474938.2013.807182},
	abstract = {This paper develops a multi-way analysis of variance for non-Gaussian multivariate distributions and provides a practical simulation algorithm to estimate the corresponding components of variance. It specifically addresses variance in Bayesian predictive distributions, showing that it may be decomposed into the sum of extrinsic variance, arising from posterior uncertainty about parameters, and intrinsic variance, which would exist even if parameters were known. Depending on the application at hand, further decomposition of extrinsic or intrinsic variance (or both) may be useful. The paper shows how to produce simulation-consistent estimates of all of these components, and the method demands little additional effort or computing time beyond that already invested in the posterior simulator. It illustrates the methods using a dynamic stochastic general equilibrium model of the US economy, both before and during the global financial crisis.},
	language = {en},
	number = {1-4},
	urldate = {2020-04-27},
	journal = {Econometric Reviews},
	author = {Geweke, John and Amisano, Gianni},
	month = feb,
	year = {2014},
	pages = {270--288},
	file = {Geweke and Amisano - 2014 - Analysis of Variance for Bayesian Inference.pdf:/home/jordy/snap/zotero-snap/common/Zotero/storage/G4AM9RBN/Geweke and Amisano - 2014 - Analysis of Variance for Bayesian Inference.pdf:application/pdf}
}

@misc{aless2019information,
    title={Where is the Information in a Deep Neural Network?},
    author={Alessandro Achille and Giovanni Paolini and Stefano Soatto},
    year={2019},
    eprint={1905.12213},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{choi_uncertainty-aware_2017,
	title = {Uncertainty-{Aware} {Learning} from {Demonstration} using {Mixture} {Density} {Networks} with {Sampling}-{Free} {Variance} {Modeling}},
	url = {http://arxiv.org/abs/1709.02249},
	abstract = {In this paper, we propose an uncertainty-aware learning from demonstration method by presenting a novel uncertainty estimation method utilizing a mixture density network appropriate for modeling complex and noisy human behaviors. The proposed uncertainty acquisition can be done with a single forward path without Monte Carlo sampling and is suitable for real-time robotics applications. The properties of the proposed uncertainty measure are analyzed through three different synthetic examples, absence of data, heavy measurement noise, and composition of functions scenarios. We show that each case can be distinguished using the proposed uncertainty measure and presented an uncertainty-aware learn- ing from demonstration method of an autonomous driving using this property. The proposed uncertainty-aware learning from demonstration method outperforms other compared methods in terms of safety using a complex real-world driving dataset.},
	urldate = {2020-04-27},
	journal = {arXiv:1709.02249 [cs]},
	author = {Choi, Sungjoon and Lee, Kyungjae and Lim, Sungbin and Oh, Songhwai},
	month = sep,
	year = {2017},
	note = {arXiv: 1709.02249},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/home/jordy/snap/zotero-snap/common/Zotero/storage/P3AZBDJP/Choi et al. - 2017 - Uncertainty-Aware Learning from Demonstration usin.pdf:application/pdf;arXiv.org Snapshot:/home/jordy/snap/zotero-snap/common/Zotero/storage/BS4ACTMC/1709.html:text/html}
}


@article{kiureghian_aleatory_2009,
	series = {Risk {Acceptance} and {Risk} {Communication}},
	title = {Aleatory or epistemic? {Does} it matter?},
	volume = {31},
	issn = {0167-4730},
	shorttitle = {Aleatory or epistemic?},
	url = {http://www.sciencedirect.com/science/article/pii/S0167473008000556},
	doi = {10.1016/j.strusafe.2008.06.020},
	abstract = {The sources and characters of uncertainties in engineering modeling for risk and reliability analyses are discussed. While many sources of uncertainty may exist, they are generally categorized as either aleatory or epistemic. Uncertainties are characterized as epistemic, if the modeler sees a possibility to reduce them by gathering more data or by refining models. Uncertainties are categorized as aleatory if the modeler does not foresee the possibility of reducing them. From a pragmatic standpoint, it is useful to thus categorize the uncertainties within a model, since it then becomes clear as to which uncertainties have the potential of being reduced. More importantly, epistemic uncertainties may introduce dependence among random events, which may not be properly noted if the character of uncertainties is not correctly modeled. Influences of the two types of uncertainties in reliability assessment, codified design, performance-based engineering and risk-based decision-making are discussed. Two simple examples demonstrate the influence of statistical dependence arising from epistemic uncertainties on systems and time-variant reliability problems.},
	language = {en},
	number = {2},
	urldate = {2020-04-28},
	journal = {Structural Safety},
	author = {Kiureghian, Armen Der and Ditlevsen, Ove},
	month = mar,
	year = {2009},
	keywords = {Aleatory, Epistemic, Ergodicity, Parameter uncertainty, Predictive models, Probability distribution choice, Statistical dependence, Systems, Time-variant reliability, Uncertainty},
	pages = {105--112},
	file = {ScienceDirect Snapshot:/home/jordy/snap/zotero-snap/common/Zotero/storage/H57BRBD6/S0167473008000556.html:text/html}
}

@article{seedat2019calibrated,
  title={Towards calibrated and scalable uncertainty representations for neural networks},
  author={Seedat, Nabeel and Kanan, Christopher},
  journal={arXiv preprint arXiv:1911.00104},
  year={2019}
}

@article{li2017dropout,
  title={{Dropout Inference in Bayesian Neural Networks with Alpha-divergences}},
  author={Li, Yingzhen and Gal, Yarin},
  journal={arXiv preprint arXiv:1703.02914},
  year={2017}
}

@misc{shridhar2018uncertainty,
    title={Uncertainty Estimations by Softplus normalization in Bayesian Convolutional Neural Networks with Variational Inference},
    author={Kumar Shridhar and Felix Laumann and Marcus Liwicki},
    year={2018},
    eprint={1806.05978},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{loshchilov_decoupled_2019,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
%url={https://openreview.net/forum?id=Bkg6RiCqY7},
}

@article{Shannon48,
  title={A Mathematical Theory of Communication},
  author={Shannon, Claude E},
  journal={The Bell System Technical Journal},
  volume={27},
  number={3},
  pages={379--423},
  year={1948},
  publisher={Nokia Bell Labs}
}

@inproceedings{guo2017calibration,
author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
title = {On Calibration of Modern Neural Networks},
year = {2017},
abstract = {Confidence calibration - the problem of predicting probability estimates representative of the true correctness likelihood - is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling - a single-parameter variant of Platt Scaling - is surprisingly effective at calibrating predictions.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {1321–1330},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{zhang2019mitigating,
    title = "Mitigating Uncertainty in Document Classification",
    author = "Zhang, Xuchao  and
      Chen, Fanglan  and
      Lu, Chang-Tien  and
      Ramakrishnan, Naren",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    % publisher = "Association for Computational Linguistics",
    % url = "https://www.aclweb.org/anthology/N19-1316",
    % doi = "10.18653/v1/N19-1316",
    pages = "3126--3136",
}

@misc{farquhar2019radial,
    title={Radial {Bayesian} Neural Networks: Beyond Discrete Support In Large-Scale {Bayesian} Deep Learning},
    author={Sebastian Farquhar and Michael Osborne and Yarin Gal},
    year={2019},
    eprint={1907.00865},
    archivePrefix={arXiv},
    primaryClass={stat.ML},
    note={arXiv:1907.00865}
}


@inproceedings{Zaragoza1998ConfidenceMF,
  title={{Confidence Measures for Neural Network Classifiers}},
  author={Zaragoza, Hugo and d’Alch{\'e}-Buc, Florence},
  booktitle={Proceedings of the Seventh International Conference Information Processing and Management of Uncertainty in Knowledge Based Systems},
  year={1998}
}

@misc{m2017distancebased,
    title={Distance-based Confidence Score for Neural Network Classifiers},
    author={Amit Mandelbaum and Daphna Weinshall},
    year={2017},
    eprint={1709.09844},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@inproceedings{gal2017concrete,
  title={{Concrete dropout}},
  author={Gal, Yarin and Hron, Jiri and Kendall, Alex},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3581--3590},
  year={2017}
}

@misc{nalisnick2018dropout,
    title={{Dropout as a Structured Shrinkage Prior}},
    author={Eric Nalisnick and José Miguel Hernández-Lobato and Padhraic Smyth},
    year={2018},
    eprint={1810.04045},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@inproceedings{kendall2017uncertainties,
  title={What uncertainties do we need in bayesian deep learning for computer vision?},
  author={Kendall, Alex and Gal, Yarin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5574--5584},
  year={2017}
}

@inproceedings{Osband2016RiskVU,
  title={Risk versus uncertainty in deep learning: Bayes, bootstrap and the dangers of dropout},
  author={Osband, Ian},
  booktitle={NIPS Workshop on Bayesian Deep Learning},
  volume={192},
  year={2016}
}

@article{ashukha_pitfalls,
  title={Pitfalls of in-domain uncertainty estimation and ensembling in deep learning},
  author={Ashukha, Arsenii and Lyzhov, Alexander and Molchanov, Dmitry and Vetrov, Dmitry},
  journal={arXiv preprint arXiv:2002.06470},
  year={2020}
}

@misc{maddox2019simple,
    title={A Simple Baseline for Bayesian Uncertainty in Deep Learning},
    author={Wesley Maddox and Timur Garipov and Pavel Izmailov and Dmitry Vetrov and Andrew Gordon Wilson},
    year={2019},
    eprint={1902.02476},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{garipov_loss_2018,
  title={Loss surfaces, mode connectivity, and fast ensembling of dnns},
  author={Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry P and Wilson, Andrew G},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8789--8798},
  year={2018}
}

@misc{huang2017snapshot,
    title={Snapshot Ensembles: Train 1, get M for free},
    author={Gao Huang and Yixuan Li and Geoff Pleiss and Zhuang Liu and John E. Hopcroft and Kilian Q. Weinberger},
    year={2017},
    eprint={1704.00109},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@book{gilks1995markov,
  added-at = {2014-03-16T11:12:07.000+0100},
  author = {Gilks, W.R. and Richardson, S. and Spiegelhalter, D.},
  biburl = {https://www.bibsonomy.org/bibtex/2193890cd11f59aab40ec3b7e54660383/peter.ralph},
  interhash = {043efb21ab1baccde979a4c67aec6e4f},
  intrahash = {193890cd11f59aab40ec3b7e54660383},
  isbn = {9780412055515},
  keywords = {MCMC reference},
  lccn = {98033429},
  publisher = {Taylor \& Francis},
  series = {Chapman \& Hall/CRC Interdisciplinary Statistics},
  timestamp = {2014-03-16T11:12:07.000+0100},
  title = {Markov Chain Monte Carlo in Practice},
  url = {http://books.google.com/books?id=TRXrMWY\_i2IC},
  year = 1995
}


@article{zhang_cyclical_2019,
	title = {Cyclical {Stochastic} {Gradient} {MCMC} for {Bayesian} {Deep} {Learning}},
	url = {http://arxiv.org/abs/1902.03932},
	abstract = {The posteriors over neural network weights are high dimensional and multimodal. Each mode typically characterizes a meaningfully different representation of the data. We develop Cyclical Stochastic Gradient MCMC (SG-MCMC) to automatically explore such distributions. In particular, we propose a cyclical stepsize schedule, where larger steps discover new modes, and smaller steps characterize each mode. We prove that our proposed learning rate schedule provides faster convergence to samples from a stationary distribution than SG-MCMC with standard decaying schedules. Moreover, we provide extensive experimental results to demonstrate the effectiveness of cyclical SG-MCMC in learning complex multimodal distributions, especially for fully Bayesian inference with modern deep neural networks.},
	urldate = {2020-04-24},
	journal = {arXiv:1902.03932 [cs, stat]},
	author = {Zhang, Ruqi and Li, Chunyuan and Zhang, Jianyi and Chen, Changyou and Wilson, Andrew Gordon},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.03932},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
	file = {arXiv Fulltext PDF:/home/jordy/snap/zotero-snap/common/Zotero/storage/4DTSSE5G/Zhang et al. - 2019 - Cyclical Stochastic Gradient MCMC for Bayesian Dee.pdf:application/pdf;arXiv.org Snapshot:/home/jordy/snap/zotero-snap/common/Zotero/storage/8CHNF9GR/1902.html:text/html}
}

@inproceedings{hoffman_langevin_2019,
	title = {Langevin {Dynamics} as {Nonparametric} {Variational} {Inference}},
	abstract = {Variational inference (VI) and Markov chain Monte Carlo (MCMC) are approximate posterior inference algorithms that are often said to have complementary strengths, with VI being fast but biased and MCMC being slower but asymptotically unbiased. In this paper, we analyze gradient-based MCMC and VI procedures and find theoretical and empirical evidence that these procedures are not as different as one might think. In particular, a close examination of the Fokker-Planck equation that governs the Langevin dynamics (LD) MCMC procedure reveals that LD implicitly follows a gradient flow that corresponds to a variational inference procedure based on optimizing a nonparametric normalizing flow. This result suggests that the transient bias of LD (due to too few warmup steps) may track that of VI (due to too few optimization steps), up to differences due to VI’s parameterization and asymptotic bias. Empirically, we find that the transient biases of these algorithms (and momentum-accelerated versions) do evolve similarly. This suggests that practitioners with a limited time budget may get more accurate results by running an MCMC procedure (even if it’s far from burned in) than a VI procedure, as long as the variance of the MCMC estimator can be dealt with (e.g., by running many parallel chains).},
	author = {Hoffman, Matt},
	year = {2019},
	file = {Full Text PDF:/home/jordy/snap/zotero-snap/common/Zotero/storage/5R3T2QNM/Hoffman - 2019 - Langevin Dynamics as Nonparametric Variational Inf.pdf:application/pdf}
}

@article{atanov_uncertainty_2018,
	title = {Uncertainty Estimation via Stochastic Batch Normalization},
	abstract = {In this work, we investigate Batch Normalization technique and propose its probabilistic interpretation. We propose a probabilistic model and show that Batch Normalization maximazes the lower bound of its marginalized log-likelihood. Then, according to the new probabilistic model, we design an algorithm which acts consistently during train and test. However, inference becomes computationally inefficient. To reduce memory and computational cost, we propose Stochastic Batch Normalization -- an efficient approximation of proper inference procedure. This method provides us with a scalable uncertainty estimation technique. We demonstrate the performance of Stochastic Batch Normalization on popular architectures (including deep convolutional architectures: VGG-like and ResNets) for MNIST and CIFAR-10 datasets.},
	urldate = {2020-04-24},
	journal = {arXiv:1802.04893 [cs, stat]},
	author = {Atanov, Andrei and Ashukha, Arsenii and Molchanov, Dmitry and Neklyudov, Kirill and Vetrov, Dmitry},
	month = mar,
	year = {2018},
	note = {arXiv: 1802.04893},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jordy/snap/zotero-snap/common/Zotero/storage/XG2WGC3Y/Atanov et al. - 2018 - Uncertainty Estimation via Stochastic Batch Normal.pdf:application/pdf;arXiv.org Snapshot:/home/jordy/snap/zotero-snap/common/Zotero/storage/KAGSX56J/1802.html:text/html}
}

@inproceedings{mccallum1999multi,
  title={Multi-label text classification with a mixture model trained by EM},
  author={McCallum, Andrew Kachites},
  booktitle={AAAI 1999 Workshop on Text Learning},
  year={1999},
  organization={Citeseer}
}

@article{mackay1995probable,
  title={Probable networks and plausible predictions—a review of practical Bayesian methods for supervised neural networks},
  author={MacKay, David JC},
  journal={Network: Computation in Neural Systems},
  volume={6},
  number={3},
  pages={469--505},
  year={1995},
  publisher={Taylor \& Francis}
}

@inproceedings{wu2016sentiment,
  title={Sentiment domain adaptation with multiple sources},
  author={Wu, Fangzhao and Huang, Yongfeng},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={301--310},
  year={2016}
}

@inproceedings{du2020adversarial,
  title={Adversarial and Domain-Aware BERT for Cross-Domain Sentiment Analysis},
  author={Du, Chunning and Sun, Haifeng and Wang, Jingyu and Qi, Qi and Liao, Jianxin},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={4019--4028},
  year={2020}
}

@article{ganin2016domain,
  title={Domain-adversarial training of neural networks},
  author={Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario and Lempitsky, Victor},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={2096--2030},
  year={2016},
  publisher={JMLR. org}
}

@inproceedings{daume2009frustratingly,
    title = "Frustratingly Easy Domain Adaptation",
    author = "Daum{\'e} III, Hal",
    booktitle = "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics",
    month = jun,
    year = "2007",
    address = "Prague, Czech Republic",
    % publisher = "Association for Computational Linguistics",
    % url = "https://www.aclweb.org/anthology/P07-1033",
    pages = "256--263",
}

@inproceedings{gururangan2018annotation,
  title={Annotation Artifacts in Natural Language Inference Data},
  author={Suchin Gururangan and Swabha Swayamdipta and Omer Levy and Roy Schwartz and Samuel R. Bowman and Noah A. Smith},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics,},
  year={2018}
}

@article{caldeira2020deeply,
  title={Deeply Uncertain: Comparing Methods of Uncertainty Quantification in Deep Learning Algorithms},
  author={Caldeira, Jo{\~a}o and Nord, Brian},
  journal={arXiv preprint arXiv:2004.10710},
  year={2020}
}

@article{scalia2020evaluating,
  title={Evaluating Scalable Uncertainty Estimation Methods for Deep Learning-Based Molecular Property Prediction},
  author={Scalia, Gabriele and Grambow, Colin A and Pernici, Barbara and Li, Yi-Pei and Green, William H},
  journal={Journal of Chemical Information and Modeling},
  year={2020},
  publisher={ACS Publications}
}

@inproceedings{joshi2012multi,
  title={Multi-domain learning: when do domains matter?},
  author={Joshi, Mahesh and Dredze, Mark and Cohen, William and Rose, Carolyn},
  booktitle={Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
  pages={1302--1312},
  year={2012}
}

@inproceedings{wen2019bayesian,
  title     = {Bayesian Uncertainty Matching for Unsupervised Domain Adaptation},
  author    = {Wen, Jun and Zheng, Nenggan and Yuan, Junsong and Gong, Zhefeng and Chen, Changyou},
  booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on
               Artificial Intelligence, {IJCAI-19}},
%   publisher = {International Joint Conferences on Artificial Intelligence Organization},             
  pages     = {3849--3855},
  year      = {2019},
  month     = {7},
%   doi       = {10.24963/ijcai.2019/534},
%   url       = {https://doi.org/10.24963/ijcai.2019/534},
}

@inproceedings{blitzer2007biographies,
  title={Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification},
  author={Blitzer, John and Dredze, Mark and Pereira, Fernando},
  booktitle={Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics},
  pages={440--447},
  year={2007}
}

@inproceedings{ziser2016neural,
    title = "Neural Structural Correspondence Learning for Domain Adaptation",
    author = "Ziser, Yftah  and
      Reichart, Roi",
    booktitle = "Proceedings of the 21st Conference on Computational Natural Language Learning",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    % publisher = "Association for Computational Linguistics",
    % url = "https://www.aclweb.org/anthology/K17-1040",
    % doi = "10.18653/v1/K17-1040",
    pages = "400--410",
}

@article{ramponi2020neural,
  title={Neural Unsupervised Domain Adaptation in NLP---A Survey},
  author={Ramponi, Alan and Plank, Barbara},
  journal={arXiv preprint arXiv:2006.00632},
  year={2020}
}

@article{emmott2015meta,
  title={A Meta-Analysis of the Anomaly Detection Problem},
  author={A. Emmott and S. Das and Thomas G. Dietterich and A. Fern and W. Wong},
  journal={arXiv: Artificial Intelligence},
  year={2015}
}

@article{teye_bayesian_2018,
	title = {Bayesian {Uncertainty} {Estimation} for {Batch} {Normalized} {Deep} {Networks}},
	url = {http://arxiv.org/abs/1802.06455},
	abstract = {We show that training a deep network using batch normalization is equivalent to approximate inference in Bayesian models. We further demonstrate that this finding allows us to make meaningful estimates of the model uncertainty using conventional architectures, without modifications to the network or the training procedure. Our approach is thoroughly validated by measuring the quality of uncertainty in a series of empirical experiments on different tasks. It outperforms baselines with strong statistical significance, and displays competitive performance with recent Bayesian approaches.},
	urldate = {2020-04-24},
	journal = {arXiv:1802.06455 [stat]},
	author = {Teye, Mattias and Azizpour, Hossein and Smith, Kevin},
	month = jul,
	year = {2018},
	note = {arXiv: 1802.06455},
	keywords = {Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jordy/snap/zotero-snap/common/Zotero/storage/RB42E7LT/Teye et al. - 2018 - Bayesian Uncertainty Estimation for Batch Normaliz.pdf:application/pdf;arXiv.org Snapshot:/home/jordy/snap/zotero-snap/common/Zotero/storage/FPHG2G49/1802.html:text/html}
}

@article{lakshminarayanan2016simple,
  title={Simple and scalable predictive uncertainty estimation using deep ensembles},
  author={Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  pages={6402--6413},
  year={2017}
}

@article{Blei_VI_2017,
   title={Variational Inference: A Review for Statisticians},
   volume={112},
   journal={Journal of the American Statistical Association},
   publisher={Informa UK Limited},
   author={Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
   year={2017},
   month={Feb},
   pages={859–877}
}

@inproceedings{ovadia2019trust,
  title={Can you Trust your Model's Uncertainty? {E}valuating Predictive Uncertainty under Dataset Shift},
  author={Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado, Zachary and Sculley, David and Nowozin, Sebastian and Dillon, Joshua and Lakshminarayanan, Balaji and Snoek, Jasper},
  booktitle={Advances in Neural Information Processing Systems},
  pages={13991--14002},
  year={2019}
}

@article{zhang_sensitivity_2016,
	title = {{A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification}},
  author={Zhang, Ye and Wallace, Byron},
  journal={arXiv preprint arXiv:1510.03820},
  year={2015}
}

@inproceedings{xiao_quantifying_2018,
	title = {Quantifying Uncertainties in Natural Language Processing Tasks},
  author={Xiao, Yijun and Wang, William Yang},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  abstract = {Reliable uncertainty quantification is a first step towards building explainable, transparent, and accountable artificial intelligent systems. Recent progress in Bayesian deep learning has made such quantification realizable. In this paper, we propose novel methods to study the benefits of characterizing model and data uncertainties for natural language processing ({NLP}) tasks. With empirical experiments on sentiment analysis, named entity recognition, and language modeling using convolutional and recurrent neural network models, we show that explicitly modeling uncertainties is not only necessary to measure output confidence levels, but also useful at enhancing model performances in various {NLP} tasks.},
  volume={33},
  pages={7322--7329},
  year={2019}
}

@article{smith_understanding_2018,
	title = {Understanding Measures of Uncertainty for Adversarial Example Detection},
	url = {http://arxiv.org/abs/1803.08533},
	abstract = {Measuring uncertainty is a promising technique for detecting adversarial examples, crafted inputs on which the model predicts an incorrect class with high confidence. But many measures of uncertainty exist, including predictive en- tropy and mutual information, each capturing different types of uncertainty. We study these measures, and shed light on why mutual information seems to be effective at the task of adversarial example detection. We highlight failure modes for {MC} dropout, a widely used approach for estimating uncertainty in deep models. This leads to an improved understanding of the drawbacks of current methods, and a proposal to improve the quality of uncertainty estimates using probabilistic model ensembles. We give illustrative experiments using {MNIST} to demonstrate the intuition underlying the different measures of uncertainty, as well as experiments on a real world Kaggle dogs vs cats classification dataset.},
	journaltitle = {{arXiv}:1803.08533 [cs, stat]},
	author = {Smith, Lewis and Gal, Yarin},
	urldate = {2020-04-17},
	date = {2018-03-22},
    year = 2018,
	eprinttype = {arxiv},
	eprint = {1803.08533},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jordy/snap/zotero-snap/common/Zotero/storage/ISKRS6WD/Smith and Gal - 2018 - Understanding Measures of Uncertainty for Adversar.pdf:application/pdf;arXiv.org Snapshot:/home/jordy/snap/zotero-snap/common/Zotero/storage/HQFUCSB9/1803.html:text/html}
}

@article{filos_benchmarking_bayes,
  title={A Systematic Comparison of Bayesian Deep Learning Robustness in Diabetic Retinopathy Tasks},
  author={Filos, Angelos and Farquhar, Sebastian and Gomez, Aidan N and Rudner, Tim GJ and Kenton, Zachary and Smith, Lewis and Alizadeh, Milad and de Kroon, Arnoud and Gal, Yarin},
  journal={arXiv preprint arXiv:1912.10481},
  year={2019}
}

@article{APTE94,
 author = "Chidanand Apt{\'{e}} and Fred Damerau and Sholom M. Weiss",
 title = {Automated Learning of Decision Rules for Text Categorization},
 journal = "ACM Transactions on Information Systems",
 year = 1994
}
 
@inproceedings{rosenthal_semeval-2017,
	title = {{SemEval-2017 Task 4: Sentiment Analysis in Twitter}},
  author={Rosenthal, Sara and Farra, Noura and Nakov, Preslav},
  booktitle={Proceedings of the 11th international workshop on semantic evaluation (SemEval-2017)},
  pages={502--518},
  year={2017}
}


@article{demsar_statistical_2006,
  author  = {Janez Dem{\v{s}}ar},
  title   = {Statistical Comparisons of Classifiers over Multiple Data Sets},
  journal = {Journal of Machine Learning Research},
  year    = {2006},
  volume  = {7},
  number  = {1},
  pages   = {1-30},
}

@misc{moejko2018inhibited,
    title={Inhibited Softmax for Uncertainty Estimation in Neural Networks},
    author={Marcin Możejko and Mateusz Susik and Rafał Karczewski},
    year={2018},
    eprint={1810.01861},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{hllermeier2019aleatoric,
    title={Aleatoric and Epistemic Uncertainty in Machine Learning: An Introduction to Concepts and Methods},
    author={Eyke Hüllermeier and Willem Waegeman},
    year={2019},
    eprint={1910.09457},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{ding2019revisiting,
    title={Revisiting the Evaluation of Uncertainty Estimation and Its Application to Explore Model Complexity-Uncertainty Trade-Off},
    author={Yukun Ding and Jinglan Liu and Jinjun Xiong and Yiyu Shi},
    year={2019},
    eprint={1903.02050},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


@article{vernekar2019outofdistribution,
    title={{Out-of-distribution Detection in Classifiers via Generation}},
  author={Vernekar, Sachin and Gaurav, Ashish and Abdelzad, Vahdat and Denouden, Taylor and Salay, Rick and Czarnecki, Krzysztof},
  journal={arXiv preprint arXiv:1910.04241},
  year={2019}
}

@misc{lorena2018complex,
    title={How Complex is your classification problem? A survey on measuring classification complexity},
    author={Ana C. Lorena and Luís P. F. Garcia and Jens Lehmann and Marcilio C. P. Souto and Tin K. Ho},
    year={2018},
    eprint={1808.03591},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014}
}

@article{amodei2016concrete,
  author    = {Dario Amodei and
               Chris Olah and
               Jacob Steinhardt and
               Paul F. Christiano and
               John Schulman and
               Dan Man{\'{e}}},
  title     = {Concrete Problems in {AI} Safety},
  journal   = {CoRR},
  volume    = {abs/1606.06565},
  year      = {2016},
 % url       = {http://arxiv.org/abs/1606.06565},
  archivePrefix = {arXiv},
  eprint    = {1606.06565},
  timestamp = {Mon, 13 Aug 2018 16:48:59 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/AmodeiOSCSM16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{brier,
author = {Brier, Glenn W.},
title = {VERIFICATION OF FORECASTS EXPRESSED IN TERMS OF PROBABILITY},
journal = {Monthly Weather Review},
volume = {78},
number = {1},
pages = {1-3},
year = {1950},
%doi = {10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2},
%URL = { https://doi.org/10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2},
eprint = { 
        https://doi.org/10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2
}
}

@article{liu2019,
author = {Liu, Han and Burnap, Pete and Alorainy, Wafa and Williams, Matthew},
year = {2019},
month = {04},
pages = {227-240},
title = {{A Fuzzy Approach to Text Classification With Two-Stage Training for Ambiguous Instances}},
volume = {6},
doi = {10.1109/TCSS.2019.2892037}
}

@inproceedings{diao_2014,
	title = {Jointly Modeling Aspects, Ratings and Sentiments for Movie Recommendation ({JMARS})},
  author={Diao, Qiming and Qiu, Minghui and Wu, Chao-Yuan and Smola, Alexander J and Jiang, Jing and Wang, Chong},
  booktitle={Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={193--202},
  year={2014}
}
